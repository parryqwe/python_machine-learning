{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多數投票法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MajorityVoteClassifier(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self,classifiers,vote='classlabel',weights=None):\n",
    "        self.classifiers=classifiers\n",
    "        self.named_classifiers={key:value for key,value in _name_estimators(classifiers)}\n",
    "        self.vote=vote\n",
    "        self.weights=weights\n",
    "    def fit(self,x,y):\n",
    "        self.lablenc_=LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        self.classes_=self.lablenc_.classes_\n",
    "        self.classifiers_=[]\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf=clone(clf).fit(x,self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "    def predict(self,x):\n",
    "        if self.vote=='probability':\n",
    "            maj_vote=np.argmax(self.predict_proba(x),axis=1)\n",
    "        else:\n",
    "            predictions=np.asarray([clf.predict(x) for clf in self.classifiers_]).T\n",
    "        maj_vote=np.array_along_axis(lamda x:np.argmax(np.bincount(x,weight=self.weights)),axis=1,arr=predictions)\n",
    "        return maj_vote\n",
    "    def predict_proba(self,x):\n",
    "        probas=np.asarray([clf.predict_proba(x) for clf in self.classifiers_])\n",
    "        avg_proba=np.average(probas,axis=0,weights=self.weights)\n",
    "        return avg_proba\n",
    "    def get_params(self,deep=True):\n",
    "        if not deep:\n",
    "            return super(MajorityVoteClassifier,self).get_params(deep=False)\n",
    "        else:\n",
    "            out=self.named_classifiers.copy()\n",
    "            for name,step in six.iteritems(self.named_classifiers):\n",
    "                for key,value in six.iteritems(step.get_params(deep=True)):\n",
    "                    out['%s__%s' % (name,key)]=value\n",
    "            return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "clf1=LogisticRegression(penalty='l2',C=0.001,random_state=0)\n",
    "clf2=DecisionTreeClassifier(max_depth=1,critierion='entropy',random_state=0)\n",
    "clf3=KNeighborsClassifier(n_neighbors=1,p=2,metric='minkowski')\n",
    "pipe1=Pipeline([['sc',StandardScaler()],['clf',clf1]])\n",
    "pipe3=Pipeline([['sc',StandardScaler()],['clf',clf3]])\n",
    "clf_labels=['Logistic Regression','Decision Tree','KNN']\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf,label in zip([pipe1,clf2,pipe3],clf_labels):\n",
    "    scores=cross_val_score(estimators=clf,x=x_train,y=y_train,cv=10,scoring='roc_auc')\n",
    "    print(\"ROC AUC:%0.2f (+/- %0.2f) [%s]\" % (scores.mean(),scores.std(),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_clf=MajorityVoteClassifier(classifiers=[pipe1,clf2,pipe3])\n",
    "clf_labels+=['Majority Voting']\n",
    "all_clf=[pipe1,clf2,pipe3,mv_clf]\n",
    "for clf,label in zip(all_clf,clf_labels):\n",
    "    scores=cross_val_score(estimator=clf,x=x_train,y=y_train,cv=10,scoring='roc_auc')\n",
    "    print(\"Accuracy:%0.2f (+/-%0.2f) [%s]\" % (scores.mean(),scores.std(),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "colors=['black','orange','blue','green']\n",
    "linestyle=[':','--','-.','-']\n",
    "for clf,label,clr,ls in zip(all_clf,clf_labels,colors,linestyles):\n",
    "    y_pred=clf.fit(x_train,y_train).predict_proba(x_test)[:,1]\n",
    "    fpr,tpr,thresholds=roc_curve(y_true=y_test,y_score=y_pred)\n",
    "    roc_auc=auc(x=fpr,y=tpr)\n",
    "    plt.plot(fpr,tpr,color=clr,linestyle=ls,label='%s (auc=%0.2f)' % (label,roc_auc))\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],linestyle='--',color='gray',linewidth=2)\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "params={'decisiontreeclassifier_max_depth':[1,2],'pipeline-1_clf_C':[0.001,0.1,100.0]}\n",
    "grid=GridSearchCV(estimator=mv_clf,param_grid=params,cv=10,scoring='roc_auc')\n",
    "grid.fit(x_train,y_train)\n",
    "for params,mean_score,scores in grid.grid_scores_:\n",
    "    print(\"%0.3f+/-%0.2f %r\" % (mean_score,scores_std() /2,params))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters:%s' % grid.best_params_)\n",
    "print('Accuracy:%.2f' % grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base_estimator:object, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators:int, default=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_samples:int or float, default=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_features:int or float, default=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bootstrap:bool, default=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bootstrap_features:bool, default=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oob_score:bool, default=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## warm_start:bool, default=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_jobs:int, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random_state:int or RandomState, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verbose:int, default=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "tree=DecisionTreeClassifier(criterion='entropy',max_depth=None)\n",
    "bag=BaggingClassifier(base_estimator=tree,n_estimators=500,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,n_jobs=1,random_state=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "tree=tree.fit(x_train,y_train)\n",
    "y_train_pred=tree.predict(x_train)\n",
    "y_test_pred=tree.predict(x_test)\n",
    "tree_train=accuracy_score(y_train,y_train_pred)\n",
    "tree_test=accuracy_score(y_test,y_test_pred)\n",
    "print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train,tree_test))\n",
    "bag=bag.fit(x_train,y_train)\n",
    "y_train_pred=bag.predict(x_train)\n",
    "y_test_pred=bag.predict(x_test)\n",
    "bag_train=accuracy_score(y_train,y_train_pred)\n",
    "bag_test=accuracy_score(y_test,y_test_pred)\n",
    "print('Bagging train/test accuracies %.3f/%.3f' % (bag_train,bag_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base_estimator:object, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators:int, default=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate:float, default=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm{‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random_state:int or RandomState, default=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "tree=DecisionTreeClassifier(criterion='entropy',max_depth=1)\n",
    "ada=AdaBoostClassifier(base_estimator=tree,n_estimators=500,learning_rate=0.1,random_state=0)\n",
    "tree=tree.fit(x_train,y_train)\n",
    "y_train_pred=tree.predict(x_train)\n",
    "y_test_pred=tree.predict(x_test)\n",
    "tree_train=accuracy_score(y_train,y_train_pred)\n",
    "tree_test=accuracy_score(y_test,y_test_pred)\n",
    "print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train,tree_test))\n",
    "ada=ada.fit(x_train,y_train)\n",
    "y_train_pred=ada.predict(x_train)\n",
    "y_test_pred=ada.predict(x_test)\n",
    "ada_train=accuracy_score(y_train,y_train_pred)\n",
    "ada_test=accuracy_score(y_test,y_test_pred)\n",
    "print('Bagging train/test accuracies %.3f/%.3f' % (ada_train,ada_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss{‘deviance’, ‘exponential’}, default=’deviance’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate:float, default=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators:int, default=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subsample:float, default=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## criterion{‘friedman_mse’, ‘mse’, ‘mae’}, default=’friedman_mse’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_samples_split:int or float, default=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_samples_leaf:int or float, default=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_weight_fraction_leaf:float, default=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_depth:int, default=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_impurity_decrease:float, default=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_impurity_split:float, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init:estimator or ‘zero’, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_features{‘auto’, ‘sqrt’, ‘log2’}, int or float, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_leaf_nodes:int, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## warm_start:bool, default=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation_fraction:float, default=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_iter_no_change:int, default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tol:float, default=1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ccp_alpha:non-negative float, default=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.predict(X_test[:2])\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "linear = LinearRegression(normalize=False, fit_intercept=True, copy_X=True)\n",
    "gdbt = GradientBoostingRegressor(tol=0.1, subsample=0.37, n_estimators=200, max_features=20, \n",
    "                                 max_depth=6, learning_rate=0.03)\n",
    "rf = RandomForestRegressor(n_estimators=300, min_samples_split=9, min_samples_leaf=10, \n",
    "                           max_features='sqrt', max_depth=8, bootstrap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## booster [default=gbtree]：選擇基分類器，gbtree: tree-based models/gblinear: linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## silent [default=0]:設定成1則沒有執行資訊輸出，最好是設定為0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nthread [default to maximum number of threads available if not set]：執行緒數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eta [default=0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_child_weight [default=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_depth [default=6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gamma [default=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_delta_step [default=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subsample [default=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colsample_bytree [default=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda [default=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha [default=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scale_pos_weight [default=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## objective [default=reg:linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval_metric [ default according to objective ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed [default=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(train_data, train_label, test_data, test_label):\n",
    "    clf = xgb.XGBClassifier(max_depth=7,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.1,\n",
    "                           n_estimators=500,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0,\n",
    "                           max_delta_step=0,\n",
    "                           subsample=1,\n",
    "                           colsample_bytree=1,\n",
    "                           colsample_bylevel=1,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=0,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=1,\n",
    "                           missing=None)\n",
    "    clf.fit(train_data, train_label, eval_metric='auc', verbose=True,\n",
    "            eval_set=[(test_data, test_label)], early_stopping_rounds=100)\n",
    "    y_pre = clf.predict(test_data)\n",
    "    y_pro = clf.predict_proba(test_data)[:, 1]\n",
    "    #print \"AUC Score : %f\" % metrics.roc_auc_score(test_label, y_pro)\n",
    "    #print\"Accuracy : %.4g\" % metrics.accuracy_score(test_label, y_pre)\n",
    "    return clf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate(eta)=automatically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## depth(max_depth)=6: 树的深度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l2_leaf_reg(reg_lambda)=3 L2正则化系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators(num_boost_round)(num_trees=1000)=1000: 解决ml问题的树的最大数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one_hot_max_size=2: 对于某些变量进行one-hot编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss_function=‘Logloss’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom_metric=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval_metric=Optimized objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan_mode=None：处理NAN的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leaf_estimation_method=None：迭代求解的方法，梯度和牛顿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random_seed=None: 训练时候的随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size = 0.25, stratify=y_train, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_index = [i for i, x in enumerate(X_train.columns) if X_train[x].dtype.kind == 'O']\n",
    "model = CatBoostClassifier(iterations = 50, learning_rate = 0.3, eval_metric='AUC', max_ctr_complexity=2, boosting_type = 'Plain', bootstrap_type= 'Bernoulli', use_best_model=True, random_seed=123)\n",
    "model.fit(X_train, y_train, cat_features=cat_features_index, eval_set=(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product, chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "def get_x(df):\n",
    "    df['Cabin'].fillna('Unknown', inplace=True)\n",
    "    df['Embarked'].fillna('Unknown', inplace=True)\n",
    "    df['Age'].fillna(-1, inplace=True)\n",
    "    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    df['Title'].fillna('na', inplace=True)\n",
    "    df = df.drop(['Name', 'PassengerId', 'Cabin', 'Embarked'], axis=1)\n",
    "    \n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = 0\n",
    "    df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    df = df.drop(['Ticket'], axis=1)\n",
    "    columns = list(df.columns)\n",
    "    if 'Survived' in columns:\n",
    "        columns.remove('Survived')\n",
    "    cat_features = np.where(df[columns].dtypes != np.float)[0]\n",
    "    return df[columns].values, cat_features\n",
    "\n",
    "\n",
    "def get_xy(df):\n",
    "    X, _ = get_x(df)\n",
    "    y = df['Survived']\n",
    "    return X, y\n",
    "\n",
    "#  \n",
    "def cross_val(X, y, X_test, param, cat_features, n_splits=3):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    acc = []\n",
    "    predict = None\n",
    "    \n",
    "    for tr_ind, val_ind in skf.split(X, y):\n",
    "        X_train = X[tr_ind]\n",
    "        y_train = y[tr_ind]\n",
    "        \n",
    "        X_valid = X[val_ind]\n",
    "        y_valid = y[val_ind]\n",
    "        \n",
    "        clf = CatBoostClassifier(iterations=500,\n",
    "                                loss_function = param['loss_function'],\n",
    "                                depth=param['depth'],\n",
    "                                l2_leaf_reg = param['l2_leaf_reg'],\n",
    "                                eval_metric = 'Accuracy',\n",
    "                                leaf_estimation_iterations = 10,\n",
    "                                use_best_model=True,\n",
    "                                logging_level='Silent'\n",
    "        )\n",
    "        \n",
    "        clf.fit(X_train, \n",
    "                y_train,\n",
    "                cat_features=cat_features,\n",
    "                eval_set=(X_valid, y_valid)\n",
    "        )\n",
    "        \n",
    "        y_pred = clf.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_valid, y_pred)\n",
    "        acc.append(accuracy)\n",
    "    return sum(acc)/n_splits\n",
    "    \n",
    "def catboost_GridSearchCV(X, y, X_test, params, cat_features, n_splits=5):\n",
    "    ps = {'acc':0,\n",
    "          'param': []\n",
    "    }\n",
    "    \n",
    "    predict=None\n",
    "    \n",
    "    for prms in tqdm(list(ParameterGrid(params)), ascii=True, desc='Params Tuning:'):\n",
    "                          \n",
    "        acc = cross_val(X, y, X_test, prms, cat_features, n_splits=5)\n",
    "\n",
    "        if acc>ps['acc']:\n",
    "            ps['acc'] = acc\n",
    "            ps['param'] = prms\n",
    "    print('Acc: '+str(ps['acc']))\n",
    "    print('Params: '+str(ps['param']))\n",
    "    \n",
    "    return ps['param']\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    train = pd.read_csv(\"../input/train.csv\")\n",
    "    test = pd.read_csv(\"../input/test.csv\")\n",
    "    \n",
    "    X_train, y_train = get_xy(train)\n",
    "    X_test, cat_features = get_x(test)\n",
    "    \n",
    "    params = {'depth':[2, 3, 4],\n",
    "              'loss_function': ['Logloss', 'CrossEntropy'],\n",
    "              'l2_leaf_reg':np.logspace(-20, -19, 3)\n",
    "    }\n",
    "    \n",
    "    param = catboost_GridSearchCV(X_train, y_train, X_test, params, cat_features)\n",
    "\n",
    "    clf = CatBoostClassifier(iterations=2500,\n",
    "                            loss_function = param['loss_function'],\n",
    "                            depth=param['depth'],\n",
    "                            l2_leaf_reg = param['l2_leaf_reg'],\n",
    "                            eval_metric = 'Accuracy',\n",
    "                            leaf_estimation_iterations = 10,\n",
    "                            use_best_model=True\n",
    "    )\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                        y_train, \n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=RANDOM_STATE,\n",
    "                                                        train_size=0.8,\n",
    "                                                        stratify=y_train\n",
    "    )\n",
    "    clf.fit(X_train, \n",
    "            y_train,\n",
    "            cat_features=cat_features,\n",
    "            logging_level='Silent',\n",
    "            eval_set=(X_valid, y_valid)\n",
    "    )\n",
    "    \n",
    "    sub = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':np.array(clf.predict(X_test)).astype(int)})\n",
    "    sub.to_csv('cat_sub_1.csv',index=False)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\t資料的用途"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## application\t模型的用途"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting要用的演算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## num_boost_round\t迭代次數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## num_leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_depth\t樹的最大深度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_data_in_leaf\t葉子可能具有的最小記錄數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_fraction\t例如 為0.8時，意味著在每次迭代中隨機選擇80％的引數來建樹\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging_fraction\t每次迭代時用的資料比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early_stopping_round\t如果一次驗證資料的一個度量在最近的early_stopping_round 回合中沒有提高，模型將停止訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda\t指定正則化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_gain_to_split\t描述分裂的最小 gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_cat_group\t在 group 邊界上找到分割點\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max_bin\t表示 feature 將存入的 bin 的最大數量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical_feature\t如果 categorical_features = 0,1,2， 則列 0，1，2是 categorical 變數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ignore_column\t與 categorical_features 類似，只不過不是將特定的列視為categorical，而是完全忽略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_binary\t這個引數為 true 時，則資料集被儲存為二進位制檔案，下次讀資料時速度會變快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "canceData=load_breast_cancer()\n",
    "X=canceData.data\n",
    "y=canceData.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.2)\n",
    "params = {    \n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'nthread':4,\n",
    "          'learning_rate':0.1,\n",
    "          'num_leaves':30, \n",
    "          'max_depth': 5,   \n",
    "          'subsample': 0.8, \n",
    "          'colsample_bytree': 0.8, \n",
    "    }\n",
    "    \n",
    "data_train = lgb.Dataset(X_train, y_train)\n",
    "cv_results = lgb.cv(params, data_train, num_boost_round=1000, nfold=5, stratified=False, shuffle=True, metrics='auc',early_stopping_rounds=50,seed=0)\n",
    "print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "print('best cv score:', pd.Series(cv_results['auc-mean']).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test1={'max_depth': range(3,8,1), 'num_leaves':range(5, 100, 5)}\n",
    "              \n",
    "gsearch1 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=188, max_depth=6, bagging_fraction = 0.8,feature_fraction = 0.8), \n",
    "                       param_grid = params_test1, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test2={'max_bin': range(5,256,10), 'min_data_in_leaf':range(1,102,10)}\n",
    "              \n",
    "gsearch2 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=188, max_depth=4, num_leaves=10,bagging_fraction = 0.8,feature_fraction = 0.8), \n",
    "                       param_grid = params_test2, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch2.fit(X_train,y_train)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test3={'feature_fraction': [0.6,0.7,0.8,0.9,1.0],\n",
    "              'bagging_fraction': [0.6,0.7,0.8,0.9,1.0],\n",
    "              'bagging_freq': range(0,81,10)\n",
    "}\n",
    "              \n",
    "gsearch3 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=188, max_depth=4, num_leaves=10,max_bin=15,min_data_in_leaf=51), \n",
    "                       param_grid = params_test3, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch3.fit(X_train,y_train)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test4={'lambda_l1': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n",
    "              'lambda_l2': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0]\n",
    "}\n",
    "              \n",
    "gsearch4 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=188, max_depth=4, num_leaves=10,max_bin=15,min_data_in_leaf=51,bagging_fraction=0.6,bagging_freq= 0, feature_fraction= 0.8), \n",
    "                       param_grid = params_test4, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch4.fit(X_train,y_train)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.01, n_estimators=1000, max_depth=4, num_leaves=10,max_bin=15,min_data_in_leaf=51,bagging_fraction=0.6,bagging_freq= 0, feature_fraction= 0.8,\n",
    "lambda_l1=1e-05,lambda_l2=1e-05,min_split_gain=0)\n",
    "model.fit(X_train,y_train)\n",
    "y_pre=model.predict(X_test)\n",
    "print(\"acc:\",metrics.accuracy_score(y_test,y_pre))\n",
    "print(\"auc:\",metrics.roc_auc_score(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=lgb.LGBMClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "y_pre=model.predict(X_test)\n",
    "print(\"acc:\",metrics.accuracy_score(y_test,y_pre))\n",
    "print(\"auc:\",metrics.roc_auc_score(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "canceData=load_breast_cancer()\n",
    "X=canceData.data\n",
    "y=canceData.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.2)\n",
    "\n",
    "### 資料轉換\n",
    "print('資料轉換')\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=False)\n",
    "\n",
    "### 設定初始引數--不含交叉驗證引數\n",
    "print('設定引數')\n",
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'nthread':4,\n",
    "          'learning_rate':0.1\n",
    "          }\n",
    "\n",
    "### 交叉驗證(調參)\n",
    "print('交叉驗證')\n",
    "max_auc = float('0')\n",
    "best_params = {}\n",
    "\n",
    "# 準確率\n",
    "print(\"調參1：提高準確率\")\n",
    "for num_leaves in range(5,100,5):\n",
    "    for max_depth in range(3,8,1):\n",
    "        params['num_leaves'] = num_leaves\n",
    "        params['max_depth'] = max_depth\n",
    "\n",
    "        cv_results = lgb.cv(\n",
    "                            params,\n",
    "                            lgb_train,\n",
    "                            seed=1,\n",
    "                            nfold=5,\n",
    "                            metrics=['auc'],\n",
    "                            early_stopping_rounds=10,\n",
    "                            verbose_eval=True\n",
    "                            )\n",
    "            \n",
    "        mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "        boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "            \n",
    "        if mean_auc >= max_auc:\n",
    "            max_auc = mean_auc\n",
    "            best_params['num_leaves'] = num_leaves\n",
    "            best_params['max_depth'] = max_depth\n",
    "if 'num_leaves' and 'max_depth' in best_params.keys():          \n",
    "    params['num_leaves'] = best_params['num_leaves']\n",
    "    params['max_depth'] = best_params['max_depth']\n",
    "\n",
    "# 過擬合\n",
    "print(\"調參2：降低過擬合\")\n",
    "for max_bin in range(5,256,10):\n",
    "    for min_data_in_leaf in range(1,102,10):\n",
    "            params['max_bin'] = max_bin\n",
    "            params['min_data_in_leaf'] = min_data_in_leaf\n",
    "            \n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=1,\n",
    "                                nfold=5,\n",
    "                                metrics=['auc'],\n",
    "                                early_stopping_rounds=10,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "                    \n",
    "            mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "            boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "            if mean_auc >= max_auc:\n",
    "                max_auc = mean_auc\n",
    "                best_params['max_bin']= max_bin\n",
    "                best_params['min_data_in_leaf'] = min_data_in_leaf\n",
    "if 'max_bin' and 'min_data_in_leaf' in best_params.keys():\n",
    "    params['min_data_in_leaf'] = best_params['min_data_in_leaf']\n",
    "    params['max_bin'] = best_params['max_bin']\n",
    "\n",
    "print(\"調參3：降低過擬合\")\n",
    "for feature_fraction in [0.6,0.7,0.8,0.9,1.0]:\n",
    "    for bagging_fraction in [0.6,0.7,0.8,0.9,1.0]:\n",
    "        for bagging_freq in range(0,50,5):\n",
    "            params['feature_fraction'] = feature_fraction\n",
    "            params['bagging_fraction'] = bagging_fraction\n",
    "            params['bagging_freq'] = bagging_freq\n",
    "            \n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=1,\n",
    "                                nfold=5,\n",
    "                                metrics=['auc'],\n",
    "                                early_stopping_rounds=10,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "                    \n",
    "            mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "            boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "            if mean_auc >= max_auc:\n",
    "                max_auc=mean_auc\n",
    "                best_params['feature_fraction'] = feature_fraction\n",
    "                best_params['bagging_fraction'] = bagging_fraction\n",
    "                best_params['bagging_freq'] = bagging_freq\n",
    "\n",
    "if 'feature_fraction' and 'bagging_fraction' and 'bagging_freq' in best_params.keys():\n",
    "    params['feature_fraction'] = best_params['feature_fraction']\n",
    "    params['bagging_fraction'] = best_params['bagging_fraction']\n",
    "    params['bagging_freq'] = best_params['bagging_freq']\n",
    "\n",
    "\n",
    "print(\"調參4：降低過擬合\")\n",
    "for lambda_l1 in [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0]:\n",
    "    for lambda_l2 in [1e-5,1e-3,1e-1,0.0,0.1,0.4,0.6,0.7,0.9,1.0]:\n",
    "        params['lambda_l1'] = lambda_l1\n",
    "        params['lambda_l2'] = lambda_l2\n",
    "        cv_results = lgb.cv(\n",
    "                            params,\n",
    "                            lgb_train,\n",
    "                            seed=1,\n",
    "                            nfold=5,\n",
    "                            metrics=['auc'],\n",
    "                            early_stopping_rounds=10,\n",
    "                            verbose_eval=True\n",
    "                            )\n",
    "                \n",
    "        mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "        boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "        if mean_auc >= max_auc:\n",
    "            max_auc=mean_auc\n",
    "            best_params['lambda_l1'] = lambda_l1\n",
    "            best_params['lambda_l2'] = lambda_l2\n",
    "if 'lambda_l1' and 'lambda_l2' in best_params.keys():\n",
    "    params['lambda_l1'] = best_params['lambda_l1']\n",
    "    params['lambda_l2'] = best_params['lambda_l2']\n",
    "\n",
    "print(\"調參5：降低過擬合2\")\n",
    "for min_split_gain in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    params['min_split_gain'] = min_split_gain\n",
    "    \n",
    "    cv_results = lgb.cv(\n",
    "                        params,\n",
    "                        lgb_train,\n",
    "                        seed=1,\n",
    "                        nfold=5,\n",
    "                        metrics=['auc'],\n",
    "                        early_stopping_rounds=10,\n",
    "                        verbose_eval=True\n",
    "                        )\n",
    "            \n",
    "    mean_auc = pd.Series(cv_results['auc-mean']).max()\n",
    "    boost_rounds = pd.Series(cv_results['auc-mean']).idxmax()\n",
    "\n",
    "    if mean_auc >= max_auc:\n",
    "        max_auc=mean_auc\n",
    "        \n",
    "        best_params['min_split_gain'] = min_split_gain\n",
    "if 'min_split_gain' in best_params.keys():\n",
    "    params['min_split_gain'] = best_params['min_split_gain']\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.01, n_estimators=1000, max_depth=4, num_leaves=10,max_bin=255,min_data_in_leaf=81,bagging_fraction=0.7,bagging_freq= 30, feature_fraction= 0.8,\n",
    "lambda_l1=0.1,lambda_l2=0,min_split_gain=0.1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pre=model.predict(X_test)\n",
    "print(\"acc:\",metrics.accuracy_score(y_test,y_pre))\n",
    "print(\"auc:\",metrics.roc_auc_score(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如果使用交叉驗證，可以將StackingClassifier都變為StackingCVClassifier即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],\n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf],\n",
    "                      ['KNN',\n",
    "                       'Random Forest',\n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "    scores = model_selection.cross_val_score(clf, X, y,\n",
    "                                             cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, sclf],\n",
    "    ['KNN',\n",
    "    'Random Forest',\n",
    "    'Naive Bayes',\n",
    "    'StackingClassifier'],\n",
    "    itertools.product([0, 1], repeat=2)):\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data  # (150,4)\n",
    "y = iris.target\n",
    "\n",
    "pipe1 = make_pipeline(ColumnSelector(cols=(0, 2)),  # 選擇第0,2列特徵\n",
    "                      LogisticRegression())\n",
    "pipe2 = make_pipeline(ColumnSelector(cols=(1, 2, 3)),  # 選擇第1,2,3列特徵\n",
    "                      LogisticRegression())\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[pipe1, pipe2],\n",
    "                          meta_classifier=LogisticRegression())\n",
    "\n",
    "sclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 堆疊泛化套件 mlxtend, 需要先行安裝(使用 pip 安裝即可)在執行環境下\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "# 因為 Stacking 需要以模型作為第一層的特徵來源, 因此在 StackingRegressor 中,\n",
    "# 除了要設本身(第二層)的判定模型 - meta_regressor, 也必須填入第一層的單模作為編碼器 - regressors\n",
    "# 這裡第二層模型(meta_regressor)的參數, 一樣也需要用 Grid/Random Search, 請參閱講義中的 mlxtrend 網頁\n",
    "meta_estimator = GradientBoostingRegressor(tol=10, subsample=0.44, n_estimators=100, \n",
    "                                           max_features='log2', max_depth=4, learning_rate=0.1)\n",
    "stacking = StackingRegressor(regressors=[linear, gdbt, rf], meta_regressor=meta_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
